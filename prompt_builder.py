"""
Promptæ„é€ æ¨¡å—
è‡ªåŠ¨è¯†åˆ«æ•°æ®å­—æ®µå¹¶æ„å»ºprompt
ä½¿ç”¨tokenizerçš„apply_chat_templateå¤„ç†æ¨¡å‹ç‰¹å®šæ ¼å¼
"""
import logging
from typing import Dict, Any, List, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class PromptBuilder:
    """è‡ªé€‚åº”çš„Promptæ„å»ºå™¨"""
    tokenizer: Any  # transformers tokenizer instance
    system_prompt: Optional[str] = None

    # å­—æ®µååˆ°å‰ç¼€çš„æ˜ å°„
    CONTEXT_FIELD_MAP = {
        'context': 'Passage',
        'passage': 'Passage',
        'story': 'Story',
        'background': 'Background',
        'document': 'Document',
        'text': 'Text',
        'paragraph': 'Paragraph',
        'article': 'Article',
        'content': 'Content',
        'source': 'Source',
        'reference': 'Reference',
    }

    QUESTION_FIELD_MAP = {
        'question': 'Question',
        'query': 'Query',
        'prompt': 'Prompt',
        'instruction': 'Instruction',
        'problem': 'Problem',
    }

    def __post_init__(self):
        # é»˜è®¤ç³»ç»Ÿæç¤º
        if self.system_prompt is None:
            self.system_prompt = "You are a helpful AI assistant. Answer questions accurately based on any provided context."

        logger.info(f"ğŸ“ åˆå§‹åŒ–è‡ªé€‚åº”PromptBuilder")

        # æ£€æŸ¥tokenizeræ˜¯å¦æ”¯æŒchat template
        if not hasattr(self.tokenizer, 'apply_chat_template'):
            logger.warning("âš ï¸ Tokenizerä¸æ”¯æŒapply_chat_templateï¼Œå°†ä½¿ç”¨fallbackæ ¼å¼")

    def build_prompt(self, data: Dict[str, Any]) -> str:
        """
        è‡ªåŠ¨è¯†åˆ«æ•°æ®å­—æ®µå¹¶æ„å»ºprompt

        Args:
            data: åŒ…å«é—®é¢˜å’Œå¯èƒ½çš„ä¸Šä¸‹æ–‡çš„å­—å…¸
                 ä¾‹å¦‚: {'question': '...', 'context': '...'}
                 æˆ–è€…: {'query': '...', 'story': '...'}

        Returns:
            æ ¼å¼åŒ–åçš„prompt
        """
        # è‡ªåŠ¨æ„å»ºç”¨æˆ·æ¶ˆæ¯
        user_message = self._auto_format_message(data)

        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": user_message}
        ]

        # ä½¿ç”¨tokenizerçš„apply_chat_template
        try:
            prompt = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
        except Exception as e:
            logger.warning(f"apply_chat_templateå¤±è´¥: {e}ï¼Œä½¿ç”¨fallback")
            prompt = self._fallback_format(messages)

        logger.debug(f"è‡ªåŠ¨æ„å»ºçš„Prompté¢„è§ˆ: {prompt[:200]}...")
        return prompt

    def _auto_format_message(self, data: Dict[str, Any]) -> str:
        """è‡ªåŠ¨è¯†åˆ«å­—æ®µå¹¶æ ¼å¼åŒ–æ¶ˆæ¯"""
        parts = []

        # 1. æŸ¥æ‰¾å¹¶æ ¼å¼åŒ–ä¸Šä¸‹æ–‡å­—æ®µ
        for field, prefix in self.CONTEXT_FIELD_MAP.items():
            if field in data and data[field]:
                parts.append(f"{prefix}: {data[field]}")
                break  # åªä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„ä¸Šä¸‹æ–‡å­—æ®µ

        # 2. æŸ¥æ‰¾å¹¶æ ¼å¼åŒ–é—®é¢˜å­—æ®µ
        for field, prefix in self.QUESTION_FIELD_MAP.items():
            if field in data and data[field]:
                parts.append(f"{prefix}: {data[field]}")
                break  # åªä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„é—®é¢˜å­—æ®µ

        # 3. å¤„ç†å…¶ä»–æœªè¯†åˆ«çš„å­—æ®µï¼ˆå¯é€‰ï¼‰
        recognized_fields = set(self.CONTEXT_FIELD_MAP.keys()) | set(self.QUESTION_FIELD_MAP.keys())
        for field, value in data.items():
            if field not in recognized_fields and value and isinstance(value, str):
                # å°†å­—æ®µåé¦–å­—æ¯å¤§å†™ä½œä¸ºå‰ç¼€
                prefix = field.capitalize()
                parts.append(f"{prefix}: {value}")

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•å­—æ®µï¼Œè¿”å›è­¦å‘Š
        if not parts:
            logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å¯è¯†åˆ«çš„å­—æ®µ")
            return "No content found"

        return "\n\n".join(parts)

    def build_prompt_simple(self, question: str, context: str = "") -> str:
        """
        ç®€å•æ¥å£ï¼šç›´æ¥ä¼ å…¥é—®é¢˜å’Œä¸Šä¸‹æ–‡
        ä¿æŒå‘åå…¼å®¹
        """
        data = {'question': question}
        if context:
            data['context'] = context
        return self.build_prompt(data)

    def _fallback_format(self, messages: List[Dict[str, str]]) -> str:
        """å¤‡ç”¨æ ¼å¼åŒ–ï¼ˆå½“tokenizerä¸æ”¯æŒchat templateæ—¶ï¼‰"""
        parts = []
        for msg in messages:
            if msg['role'] == 'system':
                parts.append(f"System: {msg['content']}")
            elif msg['role'] == 'user':
                parts.append(f"User: {msg['content']}")
        parts.append("Assistant:")
        return "\n\n".join(parts) + " "

    def detect_fields(self, data: Dict[str, Any]) -> Dict[str, str]:
        """æ£€æµ‹æ•°æ®ä¸­çš„å­—æ®µç±»å‹ï¼ˆç”¨äºè°ƒè¯•ï¼‰"""
        detected = {}

        for field in data:
            if field in self.CONTEXT_FIELD_MAP:
                detected[field] = 'context'
            elif field in self.QUESTION_FIELD_MAP:
                detected[field] = 'question'
            else:
                detected[field] = 'unknown'

        return detected


# ==================== ä¾¿æ·å‡½æ•° ====================

def create_prompt_builder(tokenizer, system_prompt: Optional[str] = None) -> PromptBuilder:
    """åˆ›å»ºè‡ªé€‚åº”Promptæ„å»ºå™¨"""
    return PromptBuilder(tokenizer, system_prompt)


# ==================== æµ‹è¯•ç¤ºä¾‹ ====================

if __name__ == "__main__":
    from transformers import AutoTokenizer

    # æµ‹è¯•å„ç§æ•°æ®æ ¼å¼
    test_data = [
        # SQuADæ ¼å¼
        {
            'context': 'The Eiffel Tower is located in Paris, France.',
            'question': 'Where is the Eiffel Tower located?'
        },
        # CoQAæ ¼å¼
        {
            'story': 'Once upon a time, there was a princess who lived in a castle.',
            'question': 'Where did the princess live?'
        },
        # DefAnæ ¼å¼ï¼ˆåªæœ‰é—®é¢˜ï¼‰
        {
            'question': 'What is machine learning?'
        },
        # è‡ªå®šä¹‰æ ¼å¼
        {
            'background': 'Python is a programming language.',
            'query': 'What is Python?'
        },
        # æ··åˆæ ¼å¼
        {
            'document': 'Climate change affects global weather patterns.',
            'prompt': 'Explain the effects mentioned.',
            'note': 'This is an additional field'  # é¢å¤–å­—æ®µä¹Ÿä¼šè¢«å¤„ç†
        }
    ]

    # ä½¿ç”¨ç¤ºä¾‹æ¨¡å‹
    tokenizer = AutoTokenizer.from_pretrained("../models/Llama-3.2-3B-Instruct")  # ä»…ä½œç¤ºä¾‹
    builder = create_prompt_builder(tokenizer)

    for i, data in enumerate(test_data):
        print(f"\n{'='*60}")
        print(f"Test case {i+1}:")
        print(f"Input data: {data}")
        print(f"Detected fields: {builder.detect_fields(data)}")

        try:
            prompt = builder.build_prompt(data)
            print(f"\nGenerated prompt:\n{prompt}")
        except Exception as e:
            print(f"Error: {e}")