"""
Chain-of-Thought å®éªŒä¸»ç¨‹åº
ä¸²è”æ‰€æœ‰æ¨¡å—ï¼Œæ‰§è¡Œå®Œæ•´çš„å®éªŒæµç¨‹
"""
import datetime
import json
import logging
import math
import os
import pickle
import queue
import threading
from dataclasses import dataclass, field
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple

import numpy as np
import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from dataset_processor import build_dataset
from prompt_builder import create_prompt_builder
from strategy_library import get_strategy_library
from utils import judge_answer

torch.set_float32_matmul_precision('high')


# def write_label_to_json(json_path, label):
#     """å¢é‡å†™å…¥nli_labelå­—æ®µ"""
#     try:
#         with open(json_path, "r", encoding="utf-8") as f:
#             data = json.load(f)
#         data['nli_label'] = label
#         with open(json_path, "w", encoding="utf-8") as f:
#             json.dump(data, f, ensure_ascii=False, indent=2)
#     except Exception as e:
#         logger.error(f"å†™å…¥nli_labelå¤±è´¥: {e}")

def write_label_to_json(json_path, label):
    """å¢é‡å†™å…¥nli_labelå­—æ®µï¼ˆä¿ç•™å…¶ä»–å­—æ®µï¼‰"""
    try:
        if os.path.exists(json_path):
            with open(json_path, "r", encoding="utf-8") as f:
                data = json.load(f)
        else:
            data = {}
        # åªæœ‰åœ¨labelå‘ç”Ÿæ”¹å˜æ—¶æ‰å†™
        if data.get('nli_label', None) != label:
            data['nli_label'] = label
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"å†™å…¥nli_labelå¤±è´¥: {e}")


# def nli_label_worker(json_path, gold_answer, model_answer, timeout=10):
#     """æ–°çº¿ç¨‹å†…åšåˆ¤æ–­ï¼Œè¶…æ—¶åˆ™å†™None"""
#     import concurrent.futures
#     label = None
#     try:
#         with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
#             future = executor.submit(judge_answer, gold_answer, model_answer)
#             label = future.result(timeout=timeout)
#     except Exception as e:
#         logger.error(f"NLIåˆ¤æ–­è¶…æ—¶æˆ–å¤±è´¥: {e}")
#         label = None
#     write_label_to_json(json_path, label)

def nli_label_worker(json_path, gold_answer, model_answer, question, timeout=10):
    import concurrent.futures
    label = None
    try:
        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(judge_answer, gold_answer, model_answer, question)
            label = future.result(timeout=timeout)
    except Exception as e:
        logger.error(f"NLIåˆ¤æ–­è¶…æ—¶æˆ–å¤±è´¥: {e}")
        label = None
    write_label_to_json(json_path, label)


# def async_nli_label(json_path, gold_answer, model_answer, timeout=10):
#     thread = threading.Thread(
#         target=nli_label_worker,
#         args=(json_path, gold_answer, model_answer, timeout)
#     )
#     thread.daemon = True
#     thread.start()
def async_nli_label(json_path, gold_answer, model_answer, question, timeout=10):
    thread = threading.Thread(
        target=nli_label_worker,
        args=(json_path, gold_answer, model_answer, question, timeout)
    )
    thread.daemon = True
    thread.start()


# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('experiment.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


@dataclass
class StepRecord:
    """æ¨ç†æ­¥éª¤è®°å½•"""
    step: int
    prompt: str  # å®Œæ•´è¾“å…¥
    response: str
    timestamp: str
    token_count: int = 0
    prompt_type: str = ""
    full_input: str = ""  # æ¨¡å‹çœ‹åˆ°çš„å®Œæ•´è¾“å…¥
    followup_prompt: str = ""  # ä»…è·Ÿè¿›æç¤ºéƒ¨åˆ†ï¼ˆçº¯æ–‡æœ¬ï¼‰
    metrics: Dict[str, Any] = field(default_factory=dict)  # å„ç§æŒ‡æ ‡


class CustomLLM:
    """è‡ªå®šä¹‰çš„LLMåŒ…è£…å™¨ï¼Œæ”¯æŒPPLè®¡ç®—å’Œè¶…æ—¶å¤„ç†"""

    def __init__(self, model, tokenizer, config):
        self.model = model
        self.tokenizer = tokenizer
        self.max_new_tokens = config.get("max_generation_tokens", 1024)
        self.temperature = config.get("temperature", 0.6)
        self.do_sample = self.temperature > 0
        self.generation_timeout = config.get("generation_timeout", 300)  # é»˜è®¤5åˆ†é’Ÿè¶…æ—¶
        self.top_p = config.get("top_p", 0.95)

    def invoke(self, prompt: str) -> str:
        """åŸºç¡€ç”Ÿæˆæ–¹æ³•ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰"""
        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True)

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=self.max_new_tokens,
                temperature=self.temperature,
                do_sample=self.do_sample,
                pad_token_id=self.tokenizer.eos_token_id,
                top_p=self.top_p,
            )

        generated_ids = outputs[0][inputs.input_ids.shape[1]:]
        return self.tokenizer.decode(generated_ids, skip_special_tokens=True)

    def _generate_with_timeout(self, inputs, result_queue, error_queue):
        """åœ¨å•ç‹¬çº¿ç¨‹ä¸­æ‰§è¡Œç”Ÿæˆ"""
        try:
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=self.max_new_tokens,
                    temperature=self.temperature,
                    do_sample=self.do_sample,
                    return_dict_in_generate=True,
                    output_scores=True,
                    output_hidden_states=True,  # æ·»åŠ è¿™ä¸€è¡Œä»¥è·å–éšè—å±‚
                    pad_token_id=self.tokenizer.eos_token_id
                )
            result_queue.put(outputs)
        except Exception as e:
            error_queue.put(e)

    def _extract_hidden_states(self, outputs, n_input_token, n_generated):
        """æå–ä¸‰ä¸ªå…³é”®ä½ç½®çš„éšè—å±‚çŠ¶æ€"""
        # è·å–éšè—å±‚
        if hasattr(outputs, 'decoder_hidden_states') and outputs.decoder_hidden_states is not None:
            hidden = outputs.decoder_hidden_states  # For encoder-decoder
        elif hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:
            hidden = outputs.hidden_states  # For decoder-only
        else:
            return (None, None, None)

        # å¤„ç†n_generatedä¸º0çš„æƒ…å†µ
        if n_generated == 0:
            logger.warning("Only stop_words were generated. For likelihoods and embeddings, taking stop word instead.")
            n_generated = 1

        # 1. last_token_embedding - ç”Ÿæˆéƒ¨åˆ†æœ€åä¸€ä¸ªtokençš„æœ€åä¸€å±‚
        if len(hidden) == 1:
            last_input = hidden[0]
        elif (n_generated - 1) >= len(hidden):
            logger.error('Taking last state because n_generated is too large')
            last_input = hidden[-1]
        else:
            last_input = hidden[n_generated - 1]

        last_layer = last_input[-1]  # æœ€åä¸€å±‚
        last_token_embedding = last_layer[:, -1, :].cpu()

        # 2. sec_last_token_embedding - ç”Ÿæˆéƒ¨åˆ†å€’æ•°ç¬¬äºŒä¸ªtokençš„æ‰€æœ‰å±‚
        if len(hidden) == 1:
            sec_last_input = hidden[0]
        elif ((n_generated - 2) >= len(hidden)):
            sec_last_input = hidden[-2] if len(hidden) >= 2 else hidden[-1]
        else:
            sec_last_input = hidden[n_generated - 2]
        sec_last_token_embedding = torch.stack([layer[:, -1, :] for layer in sec_last_input]).cpu()

        # 3. last_tok_bef_gen_embedding - ç”Ÿæˆå‰æœ€åä¸€ä¸ªtokençš„æ‰€æœ‰å±‚
        last_tok_bef_gen_input = hidden[0]  # ç¬¬ä¸€ä¸ªç”Ÿæˆæ­¥çš„hiddenåŒ…å«è¾“å…¥çš„ä¿¡æ¯
        last_tok_bef_gen_embedding = torch.stack([layer[:, -1, :] for layer in last_tok_bef_gen_input]).cpu()

        return (last_token_embedding, sec_last_token_embedding, last_tok_bef_gen_embedding)

    def invoke_with_metrics(self, prompt: str) -> Tuple[str, Dict[str, Any], Tuple]:
        """ç”Ÿæˆæ–‡æœ¬å¹¶è®¡ç®—æ¡ä»¶PPLå’Œå…¶ä»–æŒ‡æ ‡ï¼Œæ”¯æŒè¶…æ—¶ï¼Œè¿”å›éšè—å±‚"""
        device = self.model.device
        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True).to(device)
        input_length = inputs.input_ids.shape[1]

        # ä½¿ç”¨é˜Ÿåˆ—ä¼ é€’ç»“æœ
        result_queue = queue.Queue()
        error_queue = queue.Queue()

        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = datetime.datetime.now()

        # åœ¨å•ç‹¬çº¿ç¨‹ä¸­æ‰§è¡Œç”Ÿæˆ
        generation_thread = threading.Thread(
            target=self._generate_with_timeout,
            args=(inputs, result_queue, error_queue)
        )
        generation_thread.daemon = True
        generation_thread.start()

        # ç­‰å¾…ç”Ÿæˆå®Œæˆæˆ–è¶…æ—¶
        generation_thread.join(timeout=self.generation_timeout)

        if generation_thread.is_alive():
            # è¶…æ—¶å¤„ç†
            logger.warning(f"âš ï¸ ç”Ÿæˆè¶…æ—¶ï¼å·²ç­‰å¾… {self.generation_timeout} ç§’")

            # è¿”å›è¶…æ—¶å“åº”
            timeout_text = "[Generation timed out]"
            timeout_metrics = {
                "conditional_ppl": float('inf'),
                "num_tokens": 0,
                "avg_log_prob": None,
                "avg_entropy": None,
                "avg_top1_prob": None,
                "min_top1_prob": None,
                "low_conf_ratio": 1.0,
                "generation_time": self.generation_timeout,
                "timeout": True
            }
            timeout_hidden = (None, None, None)
            return timeout_text, timeout_metrics, timeout_hidden

        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
        if not error_queue.empty():
            error = error_queue.get()
            logger.error(f"ç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {error}")
            raise error

        # è·å–ç”Ÿæˆç»“æœ
        if result_queue.empty():
            raise RuntimeError("ç”Ÿæˆå®Œæˆä½†æœªè·å¾—ç»“æœ")

        outputs = result_queue.get()

        # è®¡ç®—ç”Ÿæˆæ—¶é—´
        generation_time = (datetime.datetime.now() - start_time).total_seconds()

        # è·å–ç”Ÿæˆçš„tokenå’Œscores
        generated_ids = outputs.sequences[0][input_length:]
        scores = torch.stack(outputs.scores, dim=1)[0]  # [num_generated_tokens, vocab_size]

        # è®¡ç®—æ¡ä»¶PPL
        log_probs = []
        entropies = []
        top1_probs = []

        for i, token_id in enumerate(generated_ids):
            if i < len(scores):  # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
                # è®¡ç®—log softmax
                log_softmax_scores = torch.log_softmax(scores[i], dim=-1)
                softmax_scores = torch.softmax(scores[i], dim=-1)

                # Tokençš„logæ¦‚ç‡
                log_prob = log_softmax_scores[token_id]
                log_probs.append(log_prob.item())

                # è®¡ç®—ç†µ
                mask = softmax_scores > 1e-8
                entropy = -torch.sum(softmax_scores[mask] * log_softmax_scores[mask]) if mask.any() else 0.0
                entropies.append(entropy.item())

                # Top-1æ¦‚ç‡
                top1_prob = softmax_scores.max().item()
                top1_probs.append(top1_prob)

        # è®¡ç®—PPL
        if log_probs:
            avg_log_prob = sum(log_probs) / len(log_probs)
            ppl = math.exp(-avg_log_prob)
        else:
            ppl = float('inf')

        # è§£ç æ–‡æœ¬
        generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)

        # æå–éšè—å±‚
        n_generated = len(generated_ids)
        hidden_states = self._extract_hidden_states(outputs, input_length, n_generated)

        # è®¡ç®—å…¶ä»–ç»Ÿè®¡é‡
        metrics = {
            "conditional_ppl": ppl,
            "num_tokens": len(generated_ids),
            "avg_log_prob": avg_log_prob if log_probs else None,
            "avg_entropy": sum(entropies) / len(entropies) if entropies else None,
            "avg_top1_prob": sum(top1_probs) / len(top1_probs) if top1_probs else None,
            "min_top1_prob": min(top1_probs) if top1_probs else None,
            "low_conf_ratio": sum(1 for p in top1_probs if p < 0.5) / len(top1_probs) if top1_probs else 0,
            "generation_time": generation_time,
            "timeout": False
        }

        # å¦‚æœç”Ÿæˆæ—¶é—´è¶…è¿‡30ç§’ï¼Œè®°å½•è­¦å‘Š
        if generation_time > 30:
            logger.warning(f"â±ï¸ ç”Ÿæˆè€—æ—¶è¾ƒé•¿: {generation_time:.1f} ç§’")

        return generated_text, metrics, hidden_states


class SmartTruncationManager:
    """æ™ºèƒ½æˆªæ–­ç®¡ç†å™¨"""

    def __init__(self, tokenizer, max_input_length: int):
        self.tokenizer = tokenizer
        self.max_input_length = max_input_length
        self.is_truncated = False
        self.max_recent_steps = None
        self.recent_steps: List[StepRecord] = []
        self.first_step_tokens = 0

    def add_step(self, step_record: StepRecord, first_step_text: str) -> None:
        """æ·»åŠ æ–°çš„æ¨ç†æ­¥éª¤"""
        if not self.is_truncated:
            # æœªè§¦å‘æˆªæ–­ï¼Œæ— é™å¢é•¿
            self.recent_steps.append(step_record)

            # æ£€æŸ¥æ˜¯å¦éœ€è¦è§¦å‘æˆªæ–­
            total_tokens = self._calculate_total_tokens(first_step_text)
            if total_tokens > self.max_input_length:
                logger.info(f"è§¦å‘æ™ºèƒ½æˆªæ–­ï¼šå½“å‰tokens: {total_tokens}, ä¸Šé™: {self.max_input_length}")
                self.is_truncated = True
                # è®¡ç®—èƒ½å®¹çº³çš„æœ€å¤§æ­¥æ•°ï¼ˆå½“å‰æ­¥æ•°å‡1ï¼‰
                self.max_recent_steps = len(self.recent_steps) - 1
                logger.info(f"æˆªæ–­åä¿æŒæœ€è¿‘ {self.max_recent_steps} æ­¥")
                if self.max_recent_steps > 0:
                    # ç§»é™¤æœ€æ—§çš„æ­¥éª¤
                    removed = self.recent_steps.pop(0)
                    logger.debug(f"ç§»é™¤æ­¥éª¤ {removed.step}")
        else:
            # å·²è§¦å‘æˆªæ–­ï¼Œä¿æŒå®šé•¿ï¼ˆæ ˆè¡Œä¸ºï¼‰
            self.recent_steps.append(step_record)
            if len(self.recent_steps) > self.max_recent_steps:
                removed = self.recent_steps.pop(0)  # ç§»é™¤æœ€æ—§çš„
                logger.debug(f"æ ˆæº¢å‡ºï¼Œç§»é™¤æ­¥éª¤ {removed.step}")

    def _calculate_total_tokens(self, first_step_text: str) -> int:
        """è®¡ç®—æ€»tokenæ•°"""
        total = len(self.tokenizer.encode(first_step_text))
        for step in self.recent_steps:
            total += step.token_count
        return total

    def get_truncated_steps(self) -> List[StepRecord]:
        """è·å–æˆªæ–­åçš„æ­¥éª¤åˆ—è¡¨"""
        return self.recent_steps


class CoTReasoner:
    """é“¾å¼æ€è€ƒæ¨ç†å™¨ - ä½¿ç”¨æ–°æ¨¡å—"""

    def __init__(self, config_path: str = "config.json"):
        self.config = self._load_config(config_path)
        self._setup_directories()
        self._initialize_model()
        self._setup_modules()

    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, encoding="utf-8") as f:
                config = json.load(f)
                # ç¡®ä¿åŒ…å«å¿…è¦çš„é…ç½®é¡¹
                config.setdefault("strategy", "progressive")  # å•ä¸€ç­–ç•¥å­—æ®µ
                config.setdefault("enable_ppl_detection", True)
                config.setdefault("enable_early_stopping", False)
                config.setdefault("ppl_threshold", 100)
                config.setdefault("generation_timeout", 300)
                config.setdefault("max_steps", 20)
                config.setdefault("temperature", 0.7)
                return config
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            raise

    def _setup_directories(self) -> None:
        """åˆ›å»ºè¾“å‡ºç›®å½•"""
        os.makedirs(self.config["output_dir"], exist_ok=True)

    def _initialize_model(self) -> None:
        """åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨"""
        model_path = self.config["model_path"]
        logger.info(f"ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")

        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                local_files_only=True
            )
            # self.bnb_config = BitsAndBytesConfig(
            #     load_in_4bit=True,
            #     bnb_4bit_quant_type="nf4",  # ä½ å¯ä»¥è‡ªå®šä¹‰é‡åŒ–ç±»å‹
            #     bnb_4bit_use_double_quant=True,
            #     bnb_4bit_compute_dtype=torch.bfloat16
            # )
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                local_files_only=True,
                device_map="auto",
                torch_dtype=torch.bfloat16,
                attn_implementation="flash_attention_2",
                # quantization_config=self.bnb_config,
            )

            # å¯ç”¨ PyTorch ç¼–è¯‘ï¼ˆæé«˜æ¨ç†é€Ÿåº¦ï¼‰
            self.model = torch.compile(self.model)

            # è·å–tokené™åˆ¶
            total_tokens = self._get_safe_token_limit()

            # ä¿æŒåŸæœ‰çš„è®¡ç®—é€»è¾‘
            self.max_generation_tokens = (total_tokens // 5) - 5
            self.max_input_length = total_tokens - self.max_generation_tokens

            logger.info(f"ğŸ“Š Tokené…ç½®:")
            logger.info(f"   - æ¨¡å‹æ€»é™åˆ¶: {total_tokens}")
            logger.info(f"   - æœ€å¤§ç”Ÿæˆ: {self.max_generation_tokens}")
            logger.info(f"   - æœ€å¤§è¾“å…¥: {self.max_input_length}")

            # åˆ›å»ºè‡ªå®šä¹‰LLMåŒ…è£…å™¨
            llm_config = {
                "max_generation_tokens": self.max_generation_tokens,
                "temperature": self.config.get("temperature", 0.7),
                "generation_timeout": self.config.get("generation_timeout", 300)
            }
            self.llm = CustomLLM(self.model, self.tokenizer, llm_config)
            # ======= å…³é—­æ¢¯åº¦æ£€æŸ¥ç‚¹ =======
            if hasattr(self.model, "gradient_checkpointing_disable"):
                self.model.gradient_checkpointing_disable()
            # ============================

        except Exception as e:
            logger.error(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def _get_safe_token_limit(self) -> int:
        """å®‰å…¨åœ°è·å–æ¨¡å‹çš„tokené™åˆ¶"""
        # 1. ä¼˜å…ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼
        if "model_max_tokens" in self.config:
            return self.config["model_max_tokens"]

        # 2. å°è¯•ä»tokenizerè·å–
        tokenizer_limit = getattr(self.tokenizer, 'model_max_length', None)

        # 3. æ£€æŸ¥æ˜¯å¦ä¸ºåˆç†å€¼ï¼ˆå°äº10ä¸‡ï¼‰
        if tokenizer_limit and tokenizer_limit < 100000:
            return tokenizer_limit

        # 4. å€¼ä¸åˆç†ï¼Œä½¿ç”¨é»˜è®¤å€¼
        logger.warning(f"âš ï¸ æ£€æµ‹åˆ°å¼‚å¸¸çš„tokené™åˆ¶: {tokenizer_limit}")
        logger.info("ä½¿ç”¨é€šç”¨é»˜è®¤tokené™åˆ¶: 4096")
        return 4096

    def _setup_modules(self) -> None:
        """è®¾ç½®å„ä¸ªæ¨¡å—"""
        # åˆå§‹åŒ–æ•°æ®é›†å¤„ç†å™¨
        dataset_config = {
            "dataset_path": self.config["dataset_path"],
            "sampling": self.config.get("sampling", {
                "strategy": "all"  # é»˜è®¤ä½¿ç”¨å…¨éƒ¨æ•°æ®
            })
        }

        # æ·»åŠ éšæœºç§å­ï¼ˆå¦‚æœé…ç½®ä¸­æœ‰ï¼‰
        if "random_seed" in self.config:
            dataset_config["sampling"]["seed"] = self.config["random_seed"]

        logger.info(f"ğŸ“Š æ•°æ®é›†é‡‡æ ·é…ç½®: {dataset_config['sampling']}")

        # self.dataset_processor = build_dataset(dataset_config)
        self.dataset_processor = build_dataset(
            self.config["dataset_path"],
            sampling=self.config.get("sampling", {"strategy": "all"}),
            seed=self.config.get("random_seed", 42)
        )

        # ä» processor è·å–æ£€æµ‹åˆ°çš„ç±»å‹
        # self.dataset_type = self.dataset_processor.dataset_type

        self.dataset_type = "huggingface"
        logger.info(f"ğŸ“Š æ£€æµ‹åˆ°æ•°æ®é›†ç±»å‹: {self.dataset_type}")

        # åˆå§‹åŒ–promptæ„å»ºå™¨ - ä¼ å…¥tokenizer
        self.prompt_builder = create_prompt_builder(
            self.tokenizer,
            system_prompt=self.config.get("system_prompt")  # å¯é€‰çš„ç³»ç»Ÿæç¤º
        )

        # åˆå§‹åŒ–ç­–ç•¥åº“ - ä¸éœ€è¦ä¼ å…¥tokenizer
        self.strategy_library = get_strategy_library()

        # è·å–ç­–ç•¥æ¨¡å¼
        self.strategy_mode = self.config.get("strategy", "progressive")

        # è·å–å¹¶æ˜¾ç¤ºç­–ç•¥ä¿¡æ¯
        strategy_info = self.strategy_library.get_strategy_info(self.strategy_mode)
        logger.info(f"ğŸ“‹ ä½¿ç”¨ç­–ç•¥æ¨¡å¼: {self.strategy_mode}")
        logger.info(f"   - åç§°: {strategy_info['name']}")
        logger.info(f"   - æè¿°: {strategy_info['description']}")

        # ç”¨äºè‡ªé€‚åº”ç­–ç•¥
        self._last_response_length = 0

        # ä¿å­˜åŸå§‹é—®é¢˜ï¼ˆç”¨äºæ„å»ºæ¶ˆæ¯å†å²ï¼‰
        self.original_question = None

    def get_followup_prompt(self, step: int) -> tuple[str, str]:
        """
        è·å–è·Ÿè¿›æç¤ºæ–‡æœ¬
        è¿”å›: (çº¯æ–‡æœ¬prompt, promptç±»å‹æ ‡è¯†)
        """
        # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
        context = {
            'step': step,
            'strategy_mode': self.strategy_mode,
            'last_response_length': self._last_response_length,
            # å¯ä»¥æ·»åŠ æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¦‚å†å²PPLç­‰
        }

        # ä»ç­–ç•¥åº“è·å–çº¯æ–‡æœ¬prompt
        prompt_text = self.strategy_library.get_prompt_for_context(context)

        # è¿”å›çº¯æ–‡æœ¬å’Œæ ‡è¯†
        return prompt_text, f"{self.strategy_mode}_step{step}"

    def build_conversation(self, data_item: Dict[str, Any], qa_response: str,
                           recent_steps: List[StepRecord], current_step: int) -> str:
        """æ„å»ºå¯¹è¯ - ä½¿ç”¨tokenizerçš„chat template"""

        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = []

        # æ·»åŠ åˆå§‹é—®ç­”
        messages.append({"role": "user", "content": data_item.get("question", "")})
        messages.append({"role": "assistant", "content": qa_response})

        # æ·»åŠ å†å²å¯¹è¯
        for step_record in recent_steps:
            # followup_promptæ˜¯çº¯æ–‡æœ¬
            messages.append({"role": "user", "content": step_record.followup_prompt})
            messages.append({"role": "assistant", "content": step_record.response})

        # æ·»åŠ å½“å‰æ­¥éª¤çš„æç¤º
        current_prompt_text, _ = self.get_followup_prompt(current_step)
        messages.append({"role": "user", "content": current_prompt_text})

        # ä½¿ç”¨tokenizeræ ¼å¼åŒ–æ•´ä¸ªå¯¹è¯
        if hasattr(self.tokenizer, 'apply_chat_template'):
            conversation = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
        else:
            # Fallbackï¼šç®€å•æ‹¼æ¥
            conversation = ""
            for msg in messages:
                if msg["role"] == "user":
                    conversation += f"\nUser: {msg['content']}"
                else:
                    conversation += f"\nAssistant: {msg['content']}"
            conversation += "\nAssistant:"

        return conversation

    def _calculate_quality_score(self, current_step: int, current_response: str,
                                 current_metrics: Dict[str, Any], history: List[StepRecord]) -> float:
        """
        è®¡ç®—è´¨é‡åˆ†æ•°ï¼ˆ0-100ï¼Œè¶Šé«˜è¡¨ç¤ºè¶Šå·®ï¼‰
        ä½¿ç”¨å¯¹æ•°å˜æ¢å¤„ç†æ— ç•Œå€¼
        """
        # åŸºç¡€åˆ†æ•°ç»„ä»¶
        components = []

        # 1. PPLåˆ†æ•°ï¼ˆä½¿ç”¨å¯¹æ•°å˜æ¢ï¼‰
        if 'conditional_ppl' in current_metrics:
            ppl = current_metrics['conditional_ppl']
            # å¯¹æ•°å˜æ¢ï¼šln(1+ppl) * 10ï¼Œä¸Šé™çº¦100
            ppl_score = min(100, np.log1p(ppl) * 10)
            components.append(ppl_score * 0.3)  # 30%æƒé‡

        # 2. ç½®ä¿¡åº¦åˆ†æ•°
        if 'avg_top1_prob' in current_metrics:
            # confidence = current_metrics['avg_top1_prob']
            # # ä½ç½®ä¿¡åº¦å¾—é«˜åˆ†
            # conf_score = (1 - confidence) * 100
            # components.append(conf_score * 0.2)  # 20%æƒé‡
            confidence = current_metrics.get('avg_top1_prob')
            if confidence is not None:
                conf_score = (1 - confidence) * 100
                components.append(conf_score * 0.2)

        # 3. é‡å¤åº¦åˆ†æ•°ï¼ˆåªåœ¨æœ‰å†å²æ—¶è®¡ç®—ï¼‰
        if history and current_step > 0:
            current_words = set(current_response.lower().split())

            # è®¡ç®—ä¸æœ€è¿‘3æ­¥çš„é‡å¤åº¦
            recent_words = set()
            for h in history[-3:]:
                recent_words.update(h.response.lower().split())

            if current_words and recent_words:
                overlap_ratio = len(current_words & recent_words) / len(current_words)
                repetition_score = overlap_ratio * 100
                components.append(repetition_score * 0.3)  # 30%æƒé‡

        # 4. é•¿åº¦å¼‚å¸¸åˆ†æ•°
        response_length = len(current_response.split())
        if response_length < 20:
            length_score = (1 - response_length / 20) * 100
        elif response_length > 500:
            # å¯¹æ•°å˜æ¢å¤„ç†è¶…é•¿å“åº”
            length_score = min(100, np.log(response_length / 500) * 50)
        else:
            length_score = 0
        components.append(length_score * 0.2)  # 20%æƒé‡

        # 5. è¶…æ—¶æƒ©ç½š
        if current_metrics.get('timeout', False):
            components.append(100)  # è¶…æ—¶ç›´æ¥åŠ 100åˆ†

        # ç»¼åˆåˆ†æ•°
        if components:
            quality_score = sum(components)
        else:
            quality_score = 0

        # ç¡®ä¿åœ¨0-100èŒƒå›´å†…
        return min(100, max(0, quality_score))

    def _should_early_stop(self, history: List[StepRecord]) -> bool:
        """åŸºäºmetricsåˆ¤æ–­æ˜¯å¦åº”è¯¥æ—©åœ"""
        if len(history) < 3:
            return False

        # æ£€æŸ¥æœ€è¿‘çš„PPLè¶‹åŠ¿
        recent_ppls = []
        for record in history[-3:]:
            if 'conditional_ppl' in record.metrics:
                recent_ppls.append(record.metrics['conditional_ppl'])

        if len(recent_ppls) >= 3:
            # å¦‚æœPPLè¿ç»­ä¸Šå‡ä¸”æœ€åä¸€ä¸ªè¶…è¿‡é˜ˆå€¼
            if all(recent_ppls[i] > recent_ppls[i - 1] for i in range(1, len(recent_ppls))):
                if recent_ppls[-1] > self.config.get("ppl_threshold", 100):
                    return True

        return False

    def process_question(self, data_item: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªé—®é¢˜çš„å®Œæ•´æ¨ç†é“¾"""
        question_id = data_item.get("id", "unknown")
        question = data_item.get("question", "")

        logger.info(f"\n{'=' * 60}")
        logger.info(f"â“ å¤„ç†é—®é¢˜ {question_id}")
        logger.info(f"   é—®é¢˜: {question[:100]}...")
        logger.info(f"   ç­–ç•¥: {self.strategy_mode}")
        logger.info(f"{'=' * 60}")

        # åˆå§‹åŒ–ç»“æœæ–‡ä»¶è·¯å¾„
        safe_id = str(question_id).replace(':', '_').replace('/', '_')
        result_file = os.path.join(self.config["output_dir"], f"{safe_id}.json")
        hidden_states_file = os.path.join(self.config["output_dir"], f"{safe_id}.pkl")

        # åˆå§‹åŒ–éšè—å±‚æ•°æ®å­—å…¸
        all_hidden_states = {}

        # ç¬¬0æ­¥ï¼šä½¿ç”¨prompt_builderæ„å»ºåˆå§‹QA
        initial_prompt = self.prompt_builder.build_prompt(data_item)
        logger.info(f"\nğŸ“ åˆå§‹æç¤º:\n{initial_prompt[:200]}...\n")

        # ä½¿ç”¨æ–°çš„invokeæ–¹æ³•
        if self.config.get("enable_ppl_detection", True) and hasattr(self.llm, 'invoke_with_metrics'):
            qa_answer, initial_metrics, initial_hidden = self.llm.invoke_with_metrics(initial_prompt)
            qa_answer = qa_answer.strip()
        else:
            qa_answer = self.llm.invoke(initial_prompt).strip()
            initial_metrics = {}
            initial_hidden = (None, None, None)

        # ä¿å­˜ç¬¬0æ­¥çš„éšè—å±‚
        all_hidden_states[0] = {
            'last_token_embedding': initial_hidden[0],
            'sec_last_token_embedding': initial_hidden[1],
            'last_tok_bef_gen_embedding': initial_hidden[2]
        }

        # ä¿å­˜éšè—å±‚æ•°æ®
        with open(hidden_states_file, 'wb') as f:
            pickle.dump(all_hidden_states, f)

        # è®¡ç®—åˆå§‹è´¨é‡åˆ†æ•°
        initial_quality = self._calculate_quality_score(0, qa_answer, initial_metrics, [])
        initial_metrics['quality_score'] = initial_quality

        logger.info(f"ğŸ’¬ åˆå§‹å›ç­”:\n{qa_answer[:200]}...\n")

        # å¦‚æœæœ‰PPLï¼Œè®°å½•å®ƒ
        if 'conditional_ppl' in initial_metrics:
            logger.info(f"ğŸ“Š åˆå§‹PPL: {initial_metrics['conditional_ppl']:.2f}")
        logger.info(f"ğŸ“Š åˆå§‹è´¨é‡åˆ†æ•°: {initial_quality:.2f}")

        # è®°å½•ç”Ÿæˆæ—¶é—´
        if 'generation_time' in initial_metrics:
            logger.info(f"â±ï¸ ç”Ÿæˆæ—¶é—´: {initial_metrics['generation_time']:.1f} ç§’")

        initial_qa_text = f"{initial_prompt}\n{qa_answer}"

        # åˆå§‹åŒ–å†å²è®°å½•
        history = [StepRecord(
            step=0,
            prompt=initial_prompt,
            response=qa_answer,
            timestamp=datetime.datetime.now().isoformat(),
            token_count=len(self.tokenizer.encode(initial_prompt + qa_answer)),
            prompt_type="initial_qa",
            full_input=initial_prompt,
            followup_prompt="",
            metrics=initial_metrics  # åŒ…å«quality_score
        )]

        # æ„å»ºåˆå§‹ç»“æœ
        result = self._build_result_dict(
            data_item, history,
            truncation_triggered=False,
            truncation_at_step=None
        )

        # ä¿å­˜åˆå§‹ç»“æœ
        self._save_result(result_file, result)
        logger.info(f"ğŸ’¾ å·²ä¿å­˜æ­¥éª¤ 0 çš„ç»“æœ")

        # ==== æ–°å¢ï¼šå¼‚æ­¥æ‰“nli_label ====
        gold_answer = data_item.get("answer", "")  # å‡è®¾æ•°æ®æ ‡å‡†ç­”æ¡ˆåœ¨'answer'å­—æ®µ
        model_answer = qa_answer  # æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆ
        # async_nli_label(result_file, gold_answer, model_answer, timeout=10)
        async_nli_label(result_file, gold_answer, model_answer, question, timeout=10)
        # =============================

        # åˆå§‹åŒ–æ™ºèƒ½æˆªæ–­ç®¡ç†å™¨
        truncation_manager = SmartTruncationManager(
            self.tokenizer,
            self.max_input_length
        )

        # å¤šæ­¥æ¨ç†
        for step in range(1, self.config["max_steps"] + 1):
            logger.info(f"\n{'-' * 40}")
            logger.info(f"ğŸ”„ æ­¥éª¤ {step}/{self.config['max_steps']}")

            # è·å–è·Ÿè¿›æç¤ºï¼ˆçº¯æ–‡æœ¬ï¼‰
            current_prompt_text, prompt_type = self.get_followup_prompt(step)
            logger.info(f"ğŸ“ ä½¿ç”¨æç¤º: {current_prompt_text}")

            # æ„å»ºå¯¹è¯ï¼ˆä½¿ç”¨chat templateï¼‰
            recent_steps = truncation_manager.get_truncated_steps()
            conversation = self.build_conversation(data_item, qa_answer, recent_steps, step)

            # æ£€æŸ¥å½“å‰å¯¹è¯çš„tokenæ•°
            current_tokens = len(self.tokenizer.encode(conversation))
            logger.info(f"ğŸ“Š å½“å‰è¾“å…¥tokens: {current_tokens}/{self.max_input_length} "
                        f"({'%.1f' % (current_tokens / self.max_input_length * 100)}%)")

            logger.info(f"ğŸ“ ä½¿ç”¨æç¤ºç±»å‹: {prompt_type}")

            # ç”Ÿæˆå›ç­”
            try:
                # ä½¿ç”¨æ–°çš„invokeæ–¹æ³•
                if self.config.get("enable_ppl_detection", True) and hasattr(self.llm, 'invoke_with_metrics'):
                    cot_answer, step_metrics, step_hidden = self.llm.invoke_with_metrics(conversation)
                    cot_answer = cot_answer.strip()
                else:
                    cot_answer = self.llm.invoke(conversation).strip()
                    step_metrics = {}
                    step_hidden = (None, None, None)

                # ä¿å­˜å½“å‰æ­¥çš„éšè—å±‚
                all_hidden_states[step] = {
                    'last_token_embedding': step_hidden[0],
                    'sec_last_token_embedding': step_hidden[1],
                    'last_tok_bef_gen_embedding': step_hidden[2]
                }

                # å¢é‡ä¿å­˜éšè—å±‚æ•°æ®
                with open(hidden_states_file, 'wb') as f:
                    pickle.dump(all_hidden_states, f)

                response_length = len(cot_answer.split())

                # è®¡ç®—è´¨é‡åˆ†æ•°
                quality_score = self._calculate_quality_score(step, cot_answer, step_metrics, history)
                step_metrics['quality_score'] = quality_score

                logger.info(f"ğŸ’¬ ç”Ÿæˆå›ç­” (é•¿åº¦: {response_length} words)")
                logger.debug(f"   é¢„è§ˆ: {cot_answer[:150]}...")

                # è®°å½•PPLä¿¡æ¯
                if 'conditional_ppl' in step_metrics:
                    logger.info(f"ğŸ“Š æ­¥éª¤ {step} PPL: {step_metrics['conditional_ppl']:.2f}")

                logger.info(f"ğŸ“Š æ­¥éª¤ {step} è´¨é‡åˆ†æ•°: {quality_score:.2f}")

                # è®°å½•ç”Ÿæˆæ—¶é—´
                if 'generation_time' in step_metrics:
                    logger.info(f"â±ï¸ ç”Ÿæˆæ—¶é—´: {step_metrics['generation_time']:.1f} ç§’")

                # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
                if step_metrics.get('timeout', False):
                    logger.error(f"âŒ æ­¥éª¤ {step} ç”Ÿæˆè¶…æ—¶")

                # ä¿å­˜å“åº”é•¿åº¦ä¾›è‡ªé€‚åº”ç­–ç•¥ä½¿ç”¨
                self._last_response_length = response_length

            except Exception as e:
                logger.error(f"âŒ æ­¥éª¤ {step} ç”Ÿæˆå¤±è´¥: {e}")
                # è®°å½•å¤±è´¥çš„æ­¥éª¤
                step_metrics = {
                    'error': str(e),
                    'quality_score': 100,  # é”™è¯¯å¾—æœ€é«˜åˆ†
                    'timeout': False
                }
                cot_answer = f"[Generation failed: {str(e)}]"
                response_length = 0
                self._last_response_length = 0

                # å¼‚å¸¸æ—¶ä¹Ÿä¿å­˜Noneå€¼
                all_hidden_states[step] = {
                    'last_token_embedding': None,
                    'sec_last_token_embedding': None,
                    'last_tok_bef_gen_embedding': None
                }
                with open(hidden_states_file, 'wb') as f:
                    pickle.dump(all_hidden_states, f)

            # è®¡ç®—è¿™ä¸€æ­¥çš„tokenæ•°
            step_tokens = len(self.tokenizer.encode(f"{current_prompt_text}\n{cot_answer}"))

            # åˆ›å»ºæ­¥éª¤è®°å½•
            step_record = StepRecord(
                step=step,
                prompt=conversation,
                response=cot_answer,
                timestamp=datetime.datetime.now().isoformat(),
                token_count=step_tokens,
                prompt_type=prompt_type,
                full_input=conversation,
                followup_prompt=current_prompt_text,  # çº¯æ–‡æœ¬
                metrics=step_metrics  # åŒ…å«quality_score
            )

            # æ·»åŠ åˆ°å®Œæ•´å†å²è®°å½•
            history.append(step_record)

            # æ·»åŠ åˆ°æˆªæ–­ç®¡ç†å™¨
            truncation_manager.add_step(step_record, initial_qa_text)

            # çŠ¶æ€ä¿¡æ¯
            logger.info(f"ğŸ“Œ æˆªæ–­çŠ¶æ€: {'å·²è§¦å‘' if truncation_manager.is_truncated else 'æœªè§¦å‘'}")
            if truncation_manager.is_truncated:
                logger.info(f"   ä¿æŒæœ€è¿‘ {len(truncation_manager.recent_steps)} æ­¥ "
                            f"(æœ€å¤§: {truncation_manager.max_recent_steps})")

            # å¯é€‰ï¼šåŸºäºmetricsçš„æ—©åœ
            if self.config.get("enable_early_stopping", False):
                if self._should_early_stop(history):
                    logger.info("âš ï¸ è§¦å‘æ—©åœæ¡ä»¶")
                    break

            # æ›´æ–°ç»“æœå¹¶ä¿å­˜
            result = self._build_result_dict(
                data_item, history,
                truncation_triggered=truncation_manager.is_truncated,
                truncation_at_step=truncation_manager.max_recent_steps if truncation_manager.is_truncated else None
            )

            # å¢é‡ä¿å­˜
            self._save_result(result_file, result)
            logger.info(f"ğŸ’¾ å·²ä¿å­˜æ­¥éª¤ {step} çš„ç»“æœ")

        logger.info(f"\nâœ… é—®é¢˜ {question_id} å¤„ç†å®Œæˆï¼Œå…± {len(history)} æ­¥")
        logger.info(f"ğŸ’¾ éšè—å±‚æ•°æ®å·²ä¿å­˜è‡³: {hidden_states_file}")
        return result

    def _build_result_dict(self, data_item: Dict[str, Any], history: List[StepRecord],
                           truncation_triggered: bool, truncation_at_step: Optional[int]) -> Dict[str, Any]:
        """æ„å»ºç»“æœå­—å…¸"""
        return {
            "id": data_item.get("id", "unknown"),
            "question": data_item.get("question", ""),
            "nli_label": None,  # ä¸€å¼€å§‹å°±æœ‰ï¼Œæ–¹ä¾¿è§‚å¯Ÿ
            "context_preview": data_item.get("context", "")[:200] + "..." if len(
                data_item.get("context", "")) > 200 else data_item.get("context", ""),
            "dataset_type": self.config.get("dataset_type", "unknown"),
            "model": self.config["model_path"],
            "temperature": self.config.get("temperature", 0.7),
            "strategy": self.strategy_mode,  # ç®€åŒ–çš„å­—æ®µå
            "max_steps": self.config["max_steps"],
            "actual_steps": len(history) - 1,  # ä¸åŒ…æ‹¬åˆå§‹QA
            "max_generation_tokens": self.max_generation_tokens,
            "max_input_tokens": self.max_input_length,
            "truncation_triggered": truncation_triggered,
            "truncation_at_step": truncation_at_step,
            "generation_timeout": self.config.get("generation_timeout", 300),
            "last_update": datetime.datetime.now().isoformat(),
            "reasoning_chain": [
                {
                    "step": h.step,
                    "prompt_type": h.prompt_type,
                    "full_input": h.full_input,
                    "followup_prompt": h.followup_prompt,
                    "response": h.response,
                    "timestamp": h.timestamp,
                    "token_count": h.token_count,
                    "response_length": len(h.response.split()),
                    "input_length": len(h.full_input.split()),
                    "metrics": h.metrics  # åŒ…å«æ‰€æœ‰metricså’Œquality_score
                } for h in history
            ]
        }

    # def _save_result(self, filepath: str, result: Dict[str, Any]) -> None:
    #     """å®‰å…¨åœ°ä¿å­˜ç»“æœï¼ˆåŸå­æ“ä½œï¼‰"""
    #     # å…ˆå†™å…¥ä¸´æ—¶æ–‡ä»¶
    #     temp_file = f"{filepath}.tmp"
    #     try:
    #         with open(temp_file, "w", encoding="utf-8") as f:
    #             json.dump(result, f, ensure_ascii=False, indent=2)
    #
    #         # åŸå­æ€§åœ°æ›¿æ¢åŸæ–‡ä»¶
    #         if os.path.exists(filepath):
    #             os.replace(temp_file, filepath)
    #         else:
    #             os.rename(temp_file, filepath)
    #
    #     except Exception as e:
    #         logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
    #         # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    #         if os.path.exists(temp_file):
    #             os.remove(temp_file)
    #         raise
    def _save_result(self, filepath: str, result: Dict[str, Any]) -> None:
        """å®‰å…¨åœ°ä¿å­˜ç»“æœï¼ˆåŸå­æ“ä½œï¼Œä¿ç•™å·²æœ‰nli_labelå­—æ®µï¼‰"""
        temp_file = f"{filepath}.tmp"
        try:
            # å…ˆè¯»å–ç°æœ‰labelï¼ˆå¦‚æœæœ‰ä¸”éNoneåˆ™ä¿ç•™ï¼‰
            if os.path.exists(filepath):
                with open(filepath, "r", encoding="utf-8") as f:
                    existing = json.load(f)
                if 'nli_label' in existing and existing['nli_label'] is not None:
                    result['nli_label'] = existing['nli_label']

            with open(temp_file, "w", encoding="utf-8") as f:
                json.dump(result, f, ensure_ascii=False, indent=2)

            if os.path.exists(filepath):
                os.replace(temp_file, filepath)
            else:
                os.rename(temp_file, filepath)
        except Exception as e:
            logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
            if os.path.exists(temp_file):
                os.remove(temp_file)
            raise

    def process_dataset(self) -> None:
        """å¤„ç†æ•°æ®é›†"""
        logger.info(f"ğŸ“š å¤„ç† {self.dataset_type} æ•°æ®é›†")

        # # è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
        # stats = self.dataset_processor.get_statistics()
        # logger.info(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
        # logger.info(f"   - æ€»æ¡æ•°: {stats['total_items']}")
        # logger.info(f"   - å­—æ®µ: {stats['columns']}")

        # è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
        logger.info(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
        logger.info(f"   - æ€»æ¡æ•°: {len(self.dataset_processor)}")
        logger.info(f"   - å­—æ®µ: {self.dataset_processor.column_names}")

        # åˆ›å»ºè¿›åº¦æ–‡ä»¶
        progress_file = os.path.join(self.config["output_dir"], "progress.json")

        # å¤„ç†æ¯ä¸ªé—®é¢˜
        total_items = len(self.dataset_processor)
        for idx, data_item in enumerate(tqdm(self.dataset_processor, desc="å¤„ç†é—®é¢˜")):
            try:
                logger.info(f"\n{'=' * 60}")
                logger.info(f"å¤„ç†è¿›åº¦: {idx + 1}/{total_items}")

                # æ›´æ–°è¿›åº¦
                progress = {
                    "current": idx + 1,
                    "total": total_items,
                    "percentage": (idx + 1) / total_items * 100,
                    "current_item": data_item.get("id", f"item_{idx}"),
                    "timestamp": datetime.datetime.now().isoformat()
                }
                with open(progress_file, "w") as f:
                    json.dump(progress, f, indent=2)

                result = self.process_question(data_item)

            except Exception as e:
                logger.error(f"âŒ å¤„ç†é—®é¢˜ {data_item.get('id', 'unknown')} å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()

                # ä¿å­˜é”™è¯¯ä¿¡æ¯
                error_file = os.path.join(
                    self.config["output_dir"],
                    f"{str(data_item.get('id', f'item_{idx}')).replace(':', '_').replace('/', '_')}_error.json"
                )
                error_info = {
                    "id": data_item.get("id", f"item_{idx}"),
                    "error": str(e),
                    "traceback": traceback.format_exc(),
                    "timestamp": datetime.datetime.now().isoformat()
                }
                with open(error_file, "w") as f:
                    json.dump(error_info, f, indent=2)

                continue

    def run(self) -> None:
        """è¿è¡Œä¸»ç¨‹åº"""
        logger.info(f"ğŸš€ å¼€å§‹Chain-of-Thoughtå®éªŒ")
        logger.info(f"   æ¨¡å‹: {self.config['model_path']}")
        logger.info(f"   æ•°æ®é›†: {self.config.get('dataset_type', 'unknown')}")
        logger.info(f"   ç­–ç•¥: {self.strategy_mode}")
        logger.info(f"   ç”Ÿæˆè¶…æ—¶: {self.config.get('generation_timeout', 300)} ç§’")

        start_time = datetime.datetime.now()

        try:
            self.process_dataset()
        except KeyboardInterrupt:
            logger.info("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        except Exception as e:
            logger.error(f"âŒ å¤„ç†å¤±è´¥: {e}")
            raise
        finally:
            end_time = datetime.datetime.now()
            duration = end_time - start_time
            logger.info(f"\nâœ¨ å®éªŒå®Œæˆï¼æ€»ç”¨æ—¶: {duration}")

            # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
            self._generate_summary_report()

    def _generate_summary_report(self) -> None:
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        output_dir = Path(self.config["output_dir"])
        results = []

        # æ”¶é›†æ‰€æœ‰ç»“æœ
        for json_file in output_dir.glob("*.json"):
            if json_file.name in ["summary_report.json", "progress.json"]:
                continue
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    results.append(json.load(f))
            except Exception as e:
                logger.error(f"è¯»å–ç»“æœæ–‡ä»¶ {json_file} å¤±è´¥: {e}")

        if not results:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç»“æœæ–‡ä»¶")
            return

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        summary = {
            "experiment_config": {
                "model": self.config["model_path"],
                "dataset_type": self.config.get("dataset_type", "unknown"),
                "strategy": self.strategy_mode,
                "max_steps": self.config["max_steps"],
                "temperature": self.config.get("temperature", 0.7),
                "enable_ppl_detection": self.config.get("enable_ppl_detection", True),
                "generation_timeout": self.config.get("generation_timeout", 300),
            },
            "statistics": {
                "total_questions": len(results),
                "avg_actual_steps": sum(r["actual_steps"] for r in results) / len(results),
                "truncation_rate": sum(1 for r in results if r["truncation_triggered"]) / len(results),
                "avg_final_response_length": sum(
                    r["reasoning_chain"][-1]["response_length"] for r in results
                ) / len(results),
            },
            "timestamp": datetime.datetime.now().isoformat()
        }

        # å¦‚æœå¯ç”¨äº†PPLæ£€æµ‹ï¼Œæ·»åŠ PPLç»Ÿè®¡
        if self.config.get("enable_ppl_detection", True):
            ppl_stats = self._calculate_ppl_statistics(results)
            summary["ppl_statistics"] = ppl_stats

        # æ·»åŠ è¶…æ—¶ç»Ÿè®¡
        timeout_stats = self._calculate_timeout_statistics(results)
        summary["timeout_statistics"] = timeout_stats

        # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
        summary_file = output_dir / "summary_report.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        logger.info(f"\nğŸ“Š å®éªŒæ±‡æ€»:")
        logger.info(f"   - å¤„ç†é—®é¢˜æ•°: {summary['statistics']['total_questions']}")
        logger.info(f"   - å¹³å‡æ­¥æ•°: {summary['statistics']['avg_actual_steps']:.1f}")
        logger.info(f"   - æˆªæ–­ç‡: {summary['statistics']['truncation_rate']:.1%}")
        logger.info(f"   - å¹³å‡æœ€ç»ˆå“åº”é•¿åº¦: {summary['statistics']['avg_final_response_length']:.1f} words")

        if "ppl_statistics" in summary:
            logger.info(f"\nğŸ“Š PPLç»Ÿè®¡:")
            logger.info(f"   - å¹³å‡åˆå§‹PPL: {summary['ppl_statistics']['avg_initial_ppl']:.2f}")
            logger.info(f"   - å¹³å‡æœ€ç»ˆPPL: {summary['ppl_statistics']['avg_final_ppl']:.2f}")
            logger.info(f"   - å¹³å‡PPLå¢é•¿ç‡: {summary['ppl_statistics']['avg_ppl_growth']:.2f}x")

        if "timeout_statistics" in summary:
            logger.info(f"\nâ±ï¸ è¶…æ—¶ç»Ÿè®¡:")
            logger.info(f"   - è¶…æ—¶æ­¥éª¤æ•°: {summary['timeout_statistics']['timeout_count']}")
            logger.info(f"   - è¶…æ—¶ç‡: {summary['timeout_statistics']['timeout_rate']:.1%}")
            logger.info(f"   - å¹³å‡ç”Ÿæˆæ—¶é—´: {summary['timeout_statistics']['avg_generation_time']:.1f} ç§’")

        logger.info(f"   - æŠ¥å‘Šå·²ä¿å­˜è‡³: {summary_file}")

    def _calculate_ppl_statistics(self, results: List[Dict]) -> Dict[str, float]:
        """è®¡ç®—PPLç›¸å…³çš„ç»Ÿè®¡ä¿¡æ¯"""
        initial_ppls = []
        final_ppls = []
        ppl_growths = []

        for result in results:
            chain = result["reasoning_chain"]

            # åˆå§‹PPL
            if chain and "metrics" in chain[0] and "conditional_ppl" in chain[0]["metrics"]:
                initial_ppl = chain[0]["metrics"]["conditional_ppl"]
                initial_ppls.append(initial_ppl)

                # æœ€ç»ˆPPL
                if "metrics" in chain[-1] and "conditional_ppl" in chain[-1]["metrics"]:
                    final_ppl = chain[-1]["metrics"]["conditional_ppl"]
                    final_ppls.append(final_ppl)

                    # PPLå¢é•¿ç‡
                    if initial_ppl > 0:
                        ppl_growths.append(final_ppl / initial_ppl)

        return {
            "avg_initial_ppl": sum(initial_ppls) / len(initial_ppls) if initial_ppls else 0,
            "avg_final_ppl": sum(final_ppls) / len(final_ppls) if final_ppls else 0,
            "avg_ppl_growth": sum(ppl_growths) / len(ppl_growths) if ppl_growths else 0,
            "max_final_ppl": max(final_ppls) if final_ppls else 0,
            "num_samples_with_ppl": len(initial_ppls)
        }

    def _calculate_timeout_statistics(self, results: List[Dict]) -> Dict[str, Any]:
        """è®¡ç®—è¶…æ—¶ç›¸å…³çš„ç»Ÿè®¡ä¿¡æ¯"""
        timeout_count = 0
        total_steps = 0
        generation_times = []

        for result in results:
            chain = result["reasoning_chain"]
            for step in chain:
                total_steps += 1
                if "metrics" in step:
                    if step["metrics"].get("timeout", False):
                        timeout_count += 1
                    if "generation_time" in step["metrics"] and step["metrics"]["generation_time"] is not None:
                        generation_times.append(step["metrics"]["generation_time"])

        return {
            "timeout_count": timeout_count,
            "total_steps": total_steps,
            "timeout_rate": (timeout_count / total_steps * 100) if total_steps > 0 else 0,
            "avg_generation_time": sum(generation_times) / len(generation_times) if generation_times else 0,
            "max_generation_time": max(generation_times) if generation_times else 0,
            "min_generation_time": min(generation_times) if generation_times else 0
        }


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='Chain-of-Thought Multi-Step Reasoning Experiment')
    parser.add_argument('--config', type=str, default='config.json', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    args = parser.parse_args()

    try:
        reasoner = CoTReasoner(config_path=args.config)
        reasoner.run()
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main()
