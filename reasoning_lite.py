import datetime
import json
import logging
import os
import pickle
from pathlib import Path
from typing import List, Dict, Any, Optional

import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM

# ä¿æŒè‡ªå®šä¹‰æ¨¡å—è°ƒç”¨ä¸å˜
from dataset_processor import build_dataset
from prompt_builder import create_prompt_builder
from strategy_library import get_strategy_library
from utils import judge_answer

# æ—¥å¿—é…ç½®ï¼Œå’ŒåŸæœ¬ä¸€è‡´
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('experiment.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class CoTReasoner:
    """ç°ä»£åŒ–Chain-of-Thoughtå¤šæ­¥æ¨ç†å™¨ï¼Œä»…ä¿ç•™ä¸»æµç¨‹ã€éšè—å±‚ä¿å­˜ä¸æ—¥å¿—"""

    def __init__(self, config_path: str = "config.json"):
        self.config = self._load_config(config_path)
        self._setup_directories()
        self._initialize_model()
        self._setup_modules()

    def _load_config(self, config_path: str) -> Dict[str, Any]:
        with open(config_path, encoding="utf-8") as f:
            config = json.load(f)
            config.setdefault("strategy", "progressive")
            config.setdefault("max_steps", 20)
            config.setdefault("temperature", 0.7)
            config.setdefault("output_dir", "outputs")
            return config

    def _setup_directories(self):
        os.makedirs(self.config["output_dir"], exist_ok=True)

    def _initialize_model(self):
        model_path = self.config["model_path"]
        logger.info(f"ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")

        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            local_files_only=True
        )
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            local_files_only=True,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            attn_implementation="flash_attention_2",
        )
        self.model.eval()
        # è·å–tokené™åˆ¶ï¼ˆå°½é‡è‡ªåŠ¨ï¼‰
        self.max_length = getattr(self.tokenizer, 'model_max_length', 4096)
        self.max_gen_tokens = self.config.get("max_generation_tokens", min(512, self.max_length // 5))
        logger.info(f"ğŸ“Š Tokené…ç½®: æ€»é™åˆ¶: {self.max_length} | æœ€å¤§ç”Ÿæˆ: {self.max_gen_tokens}")

    def _setup_modules(self):
        self.dataset_processor = build_dataset(
            self.config["dataset_path"],
            sampling=self.config.get("sampling", {"strategy": "all"}),
            # seed=self.config.get("random_seed", 42)
        )
        self.prompt_builder = create_prompt_builder(
            self.tokenizer,
            system_prompt=self.config.get("system_prompt")
        )
        self.strategy_library = get_strategy_library()
        self.strategy_mode = self.config.get("strategy", "progressive")
        strategy_info = self.strategy_library.get_strategy_info(self.strategy_mode)
        logger.info(f"ğŸ“‹ ä½¿ç”¨ç­–ç•¥æ¨¡å¼: {self.strategy_mode} - {strategy_info['name']}")

    def _save_hidden_states(self, hidden_dict, file_path):
        with open(file_path, 'wb') as f:
            pickle.dump(hidden_dict, f)

    def _save_result(self, file_path, result):
        temp_file = f"{file_path}.tmp"
        try:
            if os.path.exists(file_path):
                with open(file_path, "r", encoding="utf-8") as f:
                    existing = json.load(f)
                if 'nli_label' in existing and existing['nli_label'] is not None:
                    result['nli_label'] = existing['nli_label']
            with open(temp_file, "w", encoding="utf-8") as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            if os.path.exists(file_path):
                os.replace(temp_file, file_path)
            else:
                os.rename(temp_file, file_path)
        except Exception as e:
            logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
            if os.path.exists(temp_file):
                os.remove(temp_file)
            raise

    # def _extract_hidden_states(self, outputs):
    #     """
    #     outputs.hidden_states:
    #       - tuple: (total_steps = input+ç”Ÿæˆ,)
    #         æ¯ä¸ªå…ƒç´ : tuple (n_layers+1, batch, hidden)
    #     è¿”å›:
    #       - last_token_embedding: æœ€åç”Ÿæˆtokençš„æœ€åä¸€å±‚ (shape: [hidden])
    #       - sec_last_token_embedding: å€’æ•°ç¬¬äºŒtokençš„æ‰€æœ‰å±‚ (shape: [n_layers+1, hidden])
    #       - last_tok_bef_gen_embedding: è¾“å…¥æœ€åä¸€ä¸ªtokençš„æ‰€æœ‰å±‚ (shape: [n_layers+1, hidden])
    #     """
    #     hidden_states = outputs.hidden_states
    #     if not hidden_states:
    #         return (None, None, None)
    #
    #     # # 1. æœ€åç”Ÿæˆtokençš„æœ€åä¸€å±‚
    #     # last_step_hidden = hidden_states[-1]  # (n_layers+1, batch, hidden)
    #     # last_layer = last_step_hidden[-1]  # (batch, hidden)
    #     # last_token_embedding = last_layer[0].cpu()  # batch=1ï¼Œshape:[hidden]
    #
    #     # 1. æœ€åç”Ÿæˆtokençš„æ‰€æœ‰å±‚
    #     last_step_hidden = hidden_states[-1]  # (n_layers+1, batch, hidden)
    #     last_token_embedding = torch.stack([layer[0].cpu() for layer in last_step_hidden])  # [n_layers+1, hidden]
    #
    #     # 2. å€’æ•°ç¬¬äºŒç”Ÿæˆtokençš„æ‰€æœ‰å±‚
    #     if len(hidden_states) > 1:
    #         sec_last_step_hidden = hidden_states[-2]  # (n_layers+1, batch, hidden)
    #         sec_last_token_embedding = torch.stack(
    #             [layer[0].cpu() for layer in sec_last_step_hidden])  # [n_layers+1, hidden]
    #     else:
    #         sec_last_token_embedding = torch.stack([layer[0].cpu() for layer in last_step_hidden])
    #
    #     # 3. è¾“å…¥æœ€åä¸€ä¸ªtokençš„æ‰€æœ‰å±‚
    #     first_step_hidden = hidden_states[0]
    #     last_tok_bef_gen_embedding = torch.stack(
    #         [layer[0].cpu() for layer in first_step_hidden])  # [n_layers+1, hidden]
    #
    #     return (last_token_embedding, sec_last_token_embedding, last_tok_bef_gen_embedding)

    def _extract_hidden_states(self, outputs):
        """
        outputs.hidden_states:
          - tuple: (total_steps = input+ç”Ÿæˆ,)
            æ¯ä¸ªå…ƒç´ : tuple (n_layers+1, batch, seq_len, hidden)
        è¿”å›:
          - last_token_embedding: æœ€åç”Ÿæˆtokençš„æ‰€æœ‰å±‚ (shape: [n_layers+1, hidden])
          - sec_last_token_embedding: å€’æ•°ç¬¬äºŒtokençš„æ‰€æœ‰å±‚ (shape: [n_layers+1, hidden])
          - last_tok_bef_gen_embedding: è¾“å…¥æœ€åä¸€ä¸ªtokençš„æ‰€æœ‰å±‚ (shape: [n_layers+1, hidden])
        """
        hidden_states = outputs.hidden_states
        if not hidden_states:
            return (None, None, None)

        # 1. æœ€åç”Ÿæˆtokençš„æ‰€æœ‰å±‚
        last_step_hidden = hidden_states[
            -1]  # (n_layers+1, batch, seq_len, hidden) or (n_layers+1, batch, hidden) if only 1 token
        last_token_embedding = torch.stack(
            [layer[0, -1, :].cpu() for layer in last_step_hidden])  # [n_layers+1, hidden]

        # 2. å€’æ•°ç¬¬äºŒç”Ÿæˆtokençš„æ‰€æœ‰å±‚
        if len(hidden_states) > 1:
            sec_last_step_hidden = hidden_states[-2]
            sec_last_token_embedding = torch.stack([layer[0, -1, :].cpu() for layer in sec_last_step_hidden])
        else:
            sec_last_token_embedding = last_token_embedding.clone()

        # 3. è¾“å…¥æœ€åä¸€ä¸ªtokençš„æ‰€æœ‰å±‚
        first_step_hidden = hidden_states[0]
        last_tok_bef_gen_embedding = torch.stack([layer[0, -1, :].cpu() for layer in first_step_hidden])

        return (last_token_embedding, sec_last_token_embedding, last_tok_bef_gen_embedding)

    def _build_result_dict(self, data_item, chain, truncation_triggered, truncation_at_step):
        # æŒ‰åŸæ ¼å¼è¾“å‡ºï¼Œä½†æ— metricså­—æ®µ
        return {
            "id": data_item.get("id", "unknown"),
            "question": data_item.get("question", ""),
            "gold_answer": data_item.get("answer", ""),
            "nli_label": None,
            "context_preview": data_item.get("context", "")[:200] + "..." if len(data_item.get("context", "")) > 200 else data_item.get("context", ""),
            "dataset_type": self.config.get("dataset_type", "unknown"),
            "model": self.config["model_path"],
            "temperature": self.config.get("temperature", 0.7),
            "strategy": self.strategy_mode,
            "max_steps": self.config["max_steps"],
            "actual_steps": len(chain) - 1,
            "max_generation_tokens": self.max_gen_tokens,
            "max_input_tokens": self.max_length - self.max_gen_tokens,
            "truncation_triggered": truncation_triggered,
            "truncation_at_step": truncation_at_step,
            "generation_timeout": None,
            "last_update": datetime.datetime.now().isoformat(),
            "reasoning_chain": [
                {
                    "step": h['step'],
                    "prompt_type": h['prompt_type'],
                    "full_input": h['full_input'],
                    "followup_prompt": h['followup_prompt'],
                    "response": h['response'],
                    "timestamp": h['timestamp'],
                    "token_count": h['token_count'],
                    "response_length": len(h['response'].split()),
                    "input_length": len(h['full_input'].split()),
                    "metrics": {}  # å…¼å®¹æ—§ç»“æ„
                } for h in chain
            ]
        }

    def _get_followup_prompt(self, step):
        context = {
            'step': step,
            'strategy_mode': self.strategy_mode,
        }
        prompt_text = self.strategy_library.get_prompt_for_context(context)
        return prompt_text, f"{self.strategy_mode}_step{step}"

    @torch.no_grad()
    def process_question(self, data_item: Dict[str, Any]) -> Dict[str, Any]:
        question_id = data_item.get("id", "unknown")
        question = data_item.get("question", "")

        logger.info(f"\n{'=' * 60}")
        logger.info(f"â“ å¤„ç†é—®é¢˜ {question_id}")
        logger.info(f"   é—®é¢˜: {question[:100]}...")
        logger.info(f"   ç­–ç•¥: {self.strategy_mode}")

        safe_id = str(question_id).replace(':', '_').replace('/', '_')
        result_file = os.path.join(self.config["output_dir"], f"{safe_id}.json")
        hidden_states_file = os.path.join(self.config["output_dir"], f"{safe_id}.pkl")
        all_hidden_states = {}

        # ç¬¬0æ­¥
        initial_prompt = self.prompt_builder.build_prompt(data_item)
        logger.info(f"\nğŸ“ åˆå§‹æç¤º:\n{initial_prompt[:200]}...\n")
        inputs = self.tokenizer(initial_prompt, return_tensors="pt", truncation=True, max_length=self.max_length).to(self.model.device)
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=self.max_gen_tokens,
            temperature=self.config.get("temperature", 0.7),
            do_sample=(self.config.get("temperature", 0.7) > 0),
            pad_token_id=self.tokenizer.eos_token_id,
            return_dict_in_generate=True,
            output_hidden_states=True
        )
        # è§£ç 
        generated_ids = outputs.sequences[0, inputs.input_ids.shape[1]:]
        qa_answer = self.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
        initial_hidden = self._extract_hidden_states(outputs)

        all_hidden_states[0] = {
            'last_token_embedding': initial_hidden[0],
            'sec_last_token_embedding': initial_hidden[1],
            'last_tok_bef_gen_embedding': initial_hidden[2]
        }
        with open(hidden_states_file, 'wb') as f:
            pickle.dump(all_hidden_states, f)

        logger.info(f"ğŸ’¬ åˆå§‹å›ç­”:\n{qa_answer[:200]}...\n")

        # è®°å½•é“¾
        chain = [{
            'step': 0,
            'prompt_type': "initial_qa",
            'full_input': initial_prompt,
            'followup_prompt': "",
            'response': qa_answer,
            'timestamp': datetime.datetime.now().isoformat(),
            'token_count': len(self.tokenizer.encode(initial_prompt + qa_answer))
        }]

        # ä¿å­˜åˆå§‹ç»“æœ
        result = self._build_result_dict(
            data_item, chain, truncation_triggered=False, truncation_at_step=None
        )
        self._save_result(result_file, result)
        logger.info(f"ğŸ’¾ å·²ä¿å­˜æ­¥éª¤ 0 çš„ç»“æœ")

        # å¼‚æ­¥NLI
        gold_answer = data_item.get("answer", "")
        async_nli_label(result_file, gold_answer, qa_answer, question, timeout=10)

        truncation_triggered = False
        truncation_at_step = None

        for step in range(1, self.config["max_steps"] + 1):
            logger.info(f"\n{'-' * 40}")
            logger.info(f"ğŸ”„ æ­¥éª¤ {step}/{self.config['max_steps']}")
            current_prompt_text, prompt_type = self._get_followup_prompt(step)
            logger.info(f"ğŸ“ ä½¿ç”¨æç¤º: {current_prompt_text}")

            # æ„å»ºå¯¹è¯å†å²ï¼ˆåªç”¨å·²æœ‰chainçš„ç”¨æˆ·ä¸åŠ©æ‰‹è½®æµå¯¹è¯ï¼‰
            messages = [
                {"role": "user", "content": data_item.get("question", "")},
                {"role": "assistant", "content": chain[0]['response']}
            ]
            for h in chain[1:]:
                messages.append({"role": "user", "content": h['followup_prompt']})
                messages.append({"role": "assistant", "content": h['response']})
            messages.append({"role": "user", "content": current_prompt_text})

            if hasattr(self.tokenizer, 'apply_chat_template'):
                enable_thinking = self.config.get("enable_thinking", True)
                conversation = self.tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True,
                    enable_thinking=enable_thinking
                )
            else:
                conversation = ""
                for msg in messages:
                    if msg["role"] == "user":
                        conversation += f"\nUser: {msg['content']}"
                    else:
                        conversation += f"\nAssistant: {msg['content']}"
                conversation += "\nAssistant:"

            inputs = self.tokenizer(conversation, return_tensors="pt", truncation=True, max_length=self.max_length).to(self.model.device)
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=self.max_gen_tokens,
                temperature=self.config.get("temperature", 0.7),
                do_sample=(self.config.get("temperature", 0.7) > 0),
                pad_token_id=self.tokenizer.eos_token_id,
                return_dict_in_generate=True,
                output_hidden_states=True
            )
            generated_ids = outputs.sequences[0, inputs.input_ids.shape[1]:]
            cot_answer = self.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
            step_hidden = self._extract_hidden_states(outputs)

            all_hidden_states[step] = {
                'last_token_embedding': step_hidden[0],
                'sec_last_token_embedding': step_hidden[1],
                'last_tok_bef_gen_embedding': step_hidden[2]
            }
            with open(hidden_states_file, 'wb') as f:
                pickle.dump(all_hidden_states, f)

            logger.info(f"ğŸ’¬ ç”Ÿæˆå›ç­” (é•¿åº¦: {len(cot_answer.split())} words)")

            step_record = {
                'step': step,
                'prompt_type': prompt_type,
                'full_input': conversation,
                'followup_prompt': current_prompt_text,
                'response': cot_answer,
                'timestamp': datetime.datetime.now().isoformat(),
                'token_count': len(self.tokenizer.encode(f"{current_prompt_text}\n{cot_answer}"))
            }
            chain.append(step_record)

            # ä¿å­˜
            result = self._build_result_dict(
                data_item, chain, truncation_triggered, truncation_at_step
            )
            self._save_result(result_file, result)
            logger.info(f"ğŸ’¾ å·²ä¿å­˜æ­¥éª¤ {step} çš„ç»“æœ")

        logger.info(f"\nâœ… é—®é¢˜ {question_id} å¤„ç†å®Œæˆï¼Œå…± {len(chain)} æ­¥")
        logger.info(f"ğŸ’¾ éšè—å±‚æ•°æ®å·²ä¿å­˜è‡³: {hidden_states_file}")
        return result

    def process_dataset(self):
        logger.info(f"ğŸ“š å¤„ç† {self.config.get('dataset_type', 'unknown')} æ•°æ®é›†")
        logger.info(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡: æ€»æ¡æ•°: {len(self.dataset_processor)} | å­—æ®µ: {self.dataset_processor.column_names}")

        total_items = len(self.dataset_processor)
        progress_file = os.path.join(self.config["output_dir"], "progress.json")
        for idx, data_item in enumerate(tqdm(self.dataset_processor, desc="å¤„ç†é—®é¢˜")):
            try:
                logger.info(f"\n{'=' * 60}")
                logger.info(f"å¤„ç†è¿›åº¦: {idx + 1}/{total_items}")
                progress = {
                    "current": idx + 1,
                    "total": total_items,
                    "percentage": (idx + 1) / total_items * 100,
                    "current_item": data_item.get("id", f"item_{idx}"),
                    "timestamp": datetime.datetime.now().isoformat()
                }
                with open(progress_file, "w") as f:
                    json.dump(progress, f, indent=2)

                self.process_question(data_item)

            except Exception as e:
                logger.error(f"âŒ å¤„ç†é—®é¢˜ {data_item.get('id', 'unknown')} å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                error_file = os.path.join(
                    self.config["output_dir"],
                    f"{str(data_item.get('id', f'item_{idx}')).replace(':', '_').replace('/', '_')}_error.json"
                )
                error_info = {
                    "id": data_item.get("id", f"item_{idx}"),
                    "error": str(e),
                    "traceback": traceback.format_exc(),
                    "timestamp": datetime.datetime.now().isoformat()
                }
                with open(error_file, "w") as f:
                    json.dump(error_info, f, indent=2)
                continue

    def run(self):
        logger.info(f"ğŸš€ å¼€å§‹Chain-of-Thoughtå®éªŒ")
        logger.info(f"   æ¨¡å‹: {self.config['model_path']}")
        logger.info(f"   æ•°æ®é›†: {self.config.get('dataset_type', 'unknown')}")
        logger.info(f"   ç­–ç•¥: {self.strategy_mode}")
        start_time = datetime.datetime.now()
        try:
            self.process_dataset()
        except KeyboardInterrupt:
            logger.info("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        except Exception as e:
            logger.error(f"âŒ å¤„ç†å¤±è´¥: {e}")
            raise
        finally:
            duration = datetime.datetime.now() - start_time
            logger.info(f"\nâœ¨ å®éªŒå®Œæˆï¼æ€»ç”¨æ—¶: {duration}")


def async_nli_label(json_path, gold_answer, model_answer, question, timeout=10):
    import threading
    def nli_label_worker(json_path, gold_answer, model_answer, question):
        label = None
        try:
            label = judge_answer(gold_answer, model_answer, question)
        except Exception as e:
            logger.error(f"NLIåˆ¤æ–­å¤±è´¥: {e}")
        try:
            if os.path.exists(json_path):
                with open(json_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
            else:
                data = {}
            if data.get('nli_label', None) != label:
                data['nli_label'] = label
                with open(json_path, "w", encoding="utf-8") as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"å†™å…¥nli_labelå¤±è´¥: {e}")
    thread = threading.Thread(
        target=nli_label_worker,
        args=(json_path, gold_answer, model_answer, question)
    )
    thread.daemon = True
    thread.start()


def main():
    import argparse
    parser = argparse.ArgumentParser(description='Modern Chain-of-Thought Reasoning Experiment')
    parser.add_argument('--config', type=str, default='config.json', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    args = parser.parse_args()
    try:
        reasoner = CoTReasoner(config_path=args.config)
        reasoner.run()
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main()
